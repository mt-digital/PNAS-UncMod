\documentclass[letterpaper,11.5pt]{scrartcl}
\usepackage{totcount}
% \documentclass[11pt]{report}
% \documentclass{report}
% \documentclass{book}
\usepackage[bookmarks, hidelinks]{hyperref}
\usepackage{amssymb,amsmath}
\usepackage[title]{appendix}
% \usepackage{fullpage}
\usepackage{tabulary}
\usepackage{tabularx}
\usepackage{float}
% \usepackage[margin=0.50in]{geometry}
\usepackage[margin=1.00in]{geometry}

\usepackage{booktabs}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{setspace}
\doublespace
% \usepackage{url}
\usepackage{bigfoot}
\usepackage[export]{adjustbox}
\setlength\intextsep{0pt}

% Colored comments 
\usepackage{color}
\definecolor{myorange}{RGB}{240, 96, 0}
\newcommand{\mt}[1]{{\textcolor{myorange} {({\tiny MT:} #1)}}}

\definecolor{myblue}{RGB}{30,144,255}
\newcommand{\jhj}[1]{{\textcolor{myblue} {({\tiny JHJ:} #1)}}}

\definecolor{mypurple}{RGB}{148,0,211}
\newcommand{\cm}[1]{{\textcolor{mypurple} {({\tiny CM:} #1)}}}

\definecolor{mygreen}{RGB}{26, 153, 51}
\newcommand{\ps}[1]{{\textcolor{mygreen} {({\tiny PS:} #1)}}}

\newtotcounter{citnum} %From the package documentation
\def\oldbibitem{} \let\oldbibitem=\bibitem
\def\bibitem{\stepcounter{citnum}\oldbibitem}

\usepackage{graphicx}


\title{The form of uncertainty affects selection for social learning}

\author{{}}

\begin{document}
\maketitle

\newcommand{\pisub}[1]{\pi_{\mathrm{#1}}}
\newcommand{\pilow}{\pisub{low}}
\newcommand{\pihigh}{\pisub{high}}
\newcommand{\piI}{\langle \pisub{I} \rangle}
\newcommand{\piS}{\langle \pisub{S} \rangle}
\newcommand{\ledger}{\bar\pi_{ib}}

\newcommand{\meanvar}[1]{\langle #1 \rangle}
\newcommand{\meansl}{\meanvar{s}}
\newcommand{\meanpi}{\meanvar{\pi}}
\newcommand{\meansoc}{\meanvar{\pi_\mathrm{S}}}
\newcommand{\meanasoc}{\meanvar{\pi_\mathrm{A}}}
\newcommand{\meanT}{\meanvar{T}}

\newcommand{\bandit}{\text{Bandit}_b(0, 1)}

\begin{abstract}

Social learning is essential to survival. It is likely to evolve when it is more
efficient than asocial, trial-and-error learning. Theoretically, social learning
is adaptive for some uncertainty, but too much uncertainty makes
social information unreliable. This fact is empirically
supported across biology and human sciences. However, it is unclear how
specific classes and types of uncertainty affect social
learning evolution. Furthermore, existing models and experimental operationalizations of
uncertainty are ambiguously related, and 
tend to only consider a small number of behavioral
choices.  Here we use evolutionary agent-based modeling to consolidate
models of uncertainty in social learning evolution to address these issues.
We model societies of agents who perform one of a number
of possible behaviors to acquire payoffs in a time-varying environment.
We identified complex patterns in social learning evolution depending on contextual
uncertainty variables: environmental variability, ambiguous payoffs, 
decision sets size, and effective lifespans. 
Our work advances social learning evolution theory in a way that could help 
guide human, non-human, and artificial intelligence towards optimal responses 
to existential threats and new opportunities.\footnote{This document contains
\total{citnum} references.}  
\end{abstract}


\section{Introduction}

\cm{Uncertainty -> social learning widespread belief. But variously operationalized or not real uncertainty. Selling points of this paper 1) real uncertainty, 2) 4 kinds uncertainty, 3) cognition (soft-max temperature varies how much explores).}

Social learning enhances problem solving when acquiring information from others is more efficient than learning on one's own~\cite{Laland2004}. However, social learning can also lead individuals astray if, for example, they are copying outdated information. Theoretical models have helped clarify the circumstances under which social learning should be evolutionarily favored. For example, environments that change across time favor learning from others (as opposed to having a genetically fixed behavior) ~\cite{BoydRicherson1985,Henrich1998}, so long as they are not so variable that the social information reflects outdated and maladaptive behavior

point~\cite{Rogers1988,Feldman1996}. \cm{check that the citations still accurate}. This theoretical work has motivated much empirical research and helped predict behavior in several species ~\cite{McElreath2005,Kendal2018,Allen2019}. 

From this literature, the idea that \emph{uncertainty} supports social learning has proliferated, but the term often conflates several features of the environment, including temporal variability,
spatial heterogeneity, ambiguity in behavioral payoff structure, and difficulty of the task.  In translating mathematical models to verbal predictions, the formal meaning of terms like \emph{uncertainty} is often lost, facilitating such conflation. Furthermore, as the number of formal models of social learning have expanded, an ever increasing number
of modeling choices ~\cite[Figure 1]{Kendal2018} and formalizations of \emph{uncertainty} has made it difficult to compare across models and consolidate our understanding of the contexts in which why social learning is adaptive.  

To address these concerns we developed an empirically-motivated, evolutionary agent-based model that operationalizes four principal components of uncertainty. We use this integrated model to generate predictions about the kinds of uncertainty that favor social learning and to clarify the logic of why this would be the case. Answering this question is critical for understanding human behavior adapting to a rapidly changing world, both to mitigate existential threats~\cite{Moya2020,Jones2021}, and to capitalize on new opportunities such as transitioning to clean energy use and production~\cite{NatureEnergyEditorialPromisesPremises2018,Brisbois2022}.

 \cm{I moved this for now: \emph{"Furthermore, many models of social learning evolution do not account for individual-level
cognition~\cite{Heyes2016}, though humans clearly have evolved cognitive mechanisms
for dealing with uncertainty generally~\cite{Gershman2019,Schulz2019}."} because I think it interrupted the flow and didn't seem directly linked to the goal of understanding aspects of uncertainty. However, if this is one of the main features of the model that we want to highlight as an improvement over previous work, then we probably have to say something more about it. Perhaps I just wasn't seeing the connection though, and the more accurate cognitive model DOES add to our ability to study the consequences of uncertainty. If that's the case, the argument should be sharpened. If not, maybe we put this info when the cognitive model is introduced.}\mt{Agreed. I put a first draft of a better version of this paragraph closer to/at the end of the Intro.}


\mt{This paragraph is a first try to introduce the four uncertainty dimensions
with examples to illustrate the problem we are answering.
I outlined the next paragraph to get into more details of social learning:
how cultural evolution works; conformity, success-biased, etc. Cristina, please
edit this.} 
To systematically study features of uncertainty and its consequences for social learning, we identified four common classes of ecological uncertainty (``principal components'' of uncertainty) used in evolutionary models: (1) temporal environmental variability; (2) payoff
ambiguity (difference between best behavior choice and others); (3) 
selection set size (number of ecologically possible behaviors); and 
(4) the lifespan of agents (number of individual learning opportunities).
These were chosen because modeling and empirical 
studies across taxa tend use one or more of these uncertainty dimensions [Figure 1].

\cm{Other choices: softmax: existing models underestimate the power of learning from experience can improve both individual and helps social learners recover from outdated / misleading social info , no conformism: but that's ok because we're giving social learning the best chance of success 
no horizontal transmission, no spatial variation}


Temporal environmental variability is usually modeled as an independent probability of the environment changing its state each generation ~\cite{Feldman1996,McElreath2005}. The probabilities of environmental change are usually fixed ~\cite{BoydRicherson1985, Kendal2009}, but they have also been modeled as deterministic cycles ~\cite{Feldman1996} or as variable rates \cm{who does this?}. Furthermore, the consequences of environmental change can range from mild to  catastrophic. In the latter case a change of environment corresponds to no one having the adaptive behavior for the environment ~\cite{Rogers1988}. 

or may be
modeled as occurring with a fixed or variable
probability---in these examples environments
can only be in one of two states. Payoff ambiguity sometimes takes the form
of differences in expected payoffs arising from different
behaviors or strategies~\cite{Enquist2007,Rendell2010}
or by varying the standard deviation of payoffs~\cite{McElreath2005}. 
Selection set size often takes only one value: 
for example, in empirical studies of \emph{bombus terresteris} (bumble
bee)~\cite{Baracchi2018} and \emph{parus major} (great tit)~\cite{Aplin2017},
experimenters provided two behaviors for
the bees or birds to perform, respectively, each yielding greater or smaller
payoffs depending on experimental treatment. Human studies have used two
or three possible behaviors~\cite{McElreath2005,Toyokawa2019}.
Modeling studies have used two 
behaviors~\cite{Feldman1996,Rendell2010} and four behaviors~\cite{Enquist2007}. 
The age or lifespan of individuals was
found to affect on social learning in both \emph{parus
major}~\cite{Aplin2017} and \emph{poecilia reticulata} 
(guppies)~\cite{Leris2016} \mt{Need better connection between lifespan and 
age, or better references to support use of lifespan}.
Shorter lifespans mean individuals
die more uncertain of which behaviors are optimal, and pass that uncertainty on
to their offspring. To answer our research questions about the combined effect
of various forms of uncertainty on social learning evolution, we developed an
agent-based model that incorporates all four of
these uncertainty variables. We systematically vary these parameters
to understand and predict their effects on social learning evolution.

\paragraph{Social learning evolution} (CRISTINA)
To add to confusion about how to integrate diverse models of social learning
evolution, different models select different social learning components in
seemingly \emph{ad hoc} ways 
\begin{itemize}
  \item 
    Vertical, oblique, horizontal transmission
  \item
    Conformity, payoff-biased, frequency-dependent, etc. A note about how
    ``conformity'' is often not distinguished from other forms of social learning
  \item
    Review human studies
  \item
    Review non-human studies~\cite{Leris2016,Aplin2017,Avargues-Weber2018,Baracchi2018}
  \item
    One sentence on how dual-inheritance theory enables us to gloss over whether
    social learning is genetically or culturally evolved.
  \item
    More things from which we selected the operation of our model
\end{itemize}

Social learning is the adaptive icing on the cake of intelligent adaptations to ecological uncertainty. To deal with uncertainty as individuals, human experiment participants have been observed to update their tendency for exploring alternative behaviors based on how much uncertainty there is globally. Furthermore, correlations in uncertainty between potential behaviors, humans tend to adopt directed exploration strategies that favor testing behaviors with greater observed payoff variance \cite{Wilson2014,Gershman2019}. Many models of social learning assume little to nothing about the cognitive capabilities of their agents (Cristina unpack this in another paragraph, using the uncertainty studies spreadsheet?). We do not do this because \emph{blackboxing}, or ignoring, cognitive mechanisms is “bad” \cite[p. 658]{Heyes2016, Kendal2018}, but because it is theoretically meaningful. In order to understand how social learning evolves as an adaptation to uncertainty, the model is strengthened by assuming a more biologically plausible, and evolutionarily accurate, model of cognition capable of maximizing payoffs under uncertainty for an individual. In our model we do not impose any structure in randomness between behaviors, so our cognitive model is only sensitive to total randomness. Specifically, we will adopt the softmax distribution for function for selecting a behavior based on its observed payoffs–greater payoff variance means any one behavior is more likely to be selected.

\paragraph{Meaning of uncertainty in ecology (and cognitive and social sciences?) } (JAMIE, PAUL?)
We need to support our claim that these four classes of uncertainty really 
count as forms of uncertainty and show how it is consistent across ecology,
and maybe connect to uncertainty in cognitive and social sciences more broadly.

\paragraph{Evolutionary ABMs for social learning evolution theory development} (PAUL)


\section{Model}

\begin{table}[h]
    \caption{Dynamic agent-level variables. Each has an implicit time dependence.}
    \label{tab:modelParameters}
    \centering %\hspace{-3em}
    \begin{tabular}{cp{4.0in}p{1.25in}} \toprule

        Attribute & Description & Initial value \\ 

        \midrule  

        $s_i$  & Social learner trait: 1 if agent $i$ is social learner; 0 otherwise & 0
        or 1 equally likely \\

        $\bar\pi_{ib}$ & ``Ledger'': mean payoffs acquired via behavior $b$ by $i$ 
                       & $B$-vector of zeros \\

        $c_{ib}$ & Count of how many times agent $i$ performed $b$ 
              & $B$-vector of zeros \\

        $\pi_i$ & Net payoffs accumulated by $i$ within generation
                                & 0.0 \\
        \bottomrule
    \end{tabular}
\end{table}

To answer our research questions we developed an agent-based model of social
learning evolution in which a society of $N$ individuals each must decide which of
$B$ behaviors to perform at each time step within a generation of $L$ time steps. At
the end of each generation, agents are selected with replacement from the population
to reproduce another $N$ agents. Agents who inherit the social learning trait then
learn about behavioral payoffs from the previous generation, while asocial learners
begin life with no \emph{a priori} knowledge of the world.  Each behavior is a
``bandit'', a common modeling and experimental approach for representing behaviors
with probabilistic
payoffs~\cite{SuttonBartoBook,McElreath2005,Steyvers2009,Rendell2010,Schulz2019}.  Each behavior
$b$ yields Bernoulli payoffs: payoff of 1 with probability $\pi_b$ and zero payoff
with probability $1 - \pi_b$; $\pi_b$ is therefore also the expected payoff of
behavior $b$. One behavior in each generation is optimal and pays off $\pihigh$,
while the rest pay off $\pilow < \pihigh$.  Model agents must decide which behavior
to perform at each time step.  To do this, agents use an explore-exploit strategy to
sometimes try the most profitable behavior they know about, and other times try
alternatives that may pay off more reliably.  We developed a series of computational
analyses by systematically varying uncertainty parameters and observing the
frequency that model populations evolve to be social learners, the average payoffs
accrued by agents in each setting, and the time it takes for model populations to
reach fixation (agents all social or all asocial learners). Model dynamics are
illustrated in Figure~\ref{fig:schematic}.

\begin{figure}
  \caption{Agents are randomly initialized as social learners or not, with their
  payoff observations all initialized to zero (A). Then agents begin selecting
and performing behaviors and accumulating payoffs, which goes on for $L$
timesteps (B). After $L$ time steps, agents are selected to reproduce,
social learner children learn from a member of their parent's generation, and
the previous generation dies off (C). The simulation stops if children are all
social or asocial learners (i.e.\ the system reaches fixation), 
otherwise repeat another generation and evolution (3).}
  \label{fig:schematic}
  \centering
    \includegraphics[width=\textwidth]{Figures/IntraInterGenerationalDynamics.pdf}
\end{figure}


We identified four ``principal components'' of uncertainty that affect the
ability of agents to find and choose the optimal behavior: 
\emph{environmental variability}; \emph{payoff ambiguity}; \emph{selection set size}
(number of behaviors environment affords); and the \emph{effective lifespan}.
Environmental variability, denoted $u$, is the probability that the behavior
yielding the optimal payoff, $\pihigh$, changes from one generation to the next.
We operationalized payoff ambiguity by setting $\pihigh = 0.9$ for all analyses
and varying $\pilow \in \{0.1, 0.45, 0.8\}$. The selection set size is the
total number of possible behaviors agents choose from, denoted $B$. The 
effective life span is the number of time steps agents live for, which is
also the number of behaviors each agent performs each generation.

\vspace{2em}
\begin{table}[h]
    \caption{Global environmental and cognitive initialization parameters.
      Bold indicates \emph{default value tested}.
    \mt{TODO: Test more $N_\text{T}$ for sensitivity analysis}}
    \label{tab:modelParameters}
    \centering %\hspace{-3em}
    \begin{tabular}{cp{4.0in}p{1.25in}} \toprule

        Symbol & Description & Values tested \\ 

        \midrule  

        $B$       & Number of possible behaviors (represented by ``bandits'') 
                  & 2, 4, 10 \\

        $\pihigh$ & Probability that the unique optimal behavior pays off 1 
                & \textbf{0.9} \\

        $\pilow$ & Probability one of $B - 1$ non-optimal behaviors pays off 1 
                 & 0.1, 0.45, 0.8 \\ 

        $\tau$ & Softmax temp.; $\uparrow=$more exploration, $\downarrow=$more
                    exploitation 
               & 0.01, \textbf{0.1}, 1.0 \\
        
        $u$    & Probability optimal behavior changes between generations 
               & 0.0, 0.1, \ldots, 1.0 \\

        $L$    & Number of time steps per generation & 1, $B/2$, $B$, $2B$, $4B$ \\

        $N_T$    & Number of teachers to pool, from which best selected 
                 & \textbf{5}  \\

            
               
        \bottomrule
        \end{tabular} 
\end{table}


To guide behavior selection, agent $i$ counts how many times it
performed each behavior $b$, denoted $c_{ib}$. Agents use this count to 
update the mean payoff they have obtained from behavior $b$, denoted $\ledger$.
$\ledger$ is initialized to 0 for all $b$ at model start for
all agents (Figure~\ref{fig:schematic}A), and at 
generation start for asocial learners. Social
learning agents' $\ledger$ is initialized as that of its teacher, who is selected
through performance-biased oblique learning; $c_ib$ is set to 1 for all social
learners $i$ and behaviors $b$ (Figure~\ref{fig:schematic}C). Agent $i$'s
accumulated payoffs from performing several behaviors over their lifetime of $L$
time steps is denoted $\pi_{i}$. If an agent $i$ is a social learner, then we write
$s_i = 1$, otherwise the agent is an asocial learner and $s_i = 0$.

At generation start, one of the behaviors is set to 
yield expected payoff $\pihigh$, with the other $B-1$ behaviors yielding
expected payoffs $\pilow$. At each time step, agents perform behavior $b$ 
with softmax-weighted probability
\begin{equation}
  \Pr(\text{Agent $i$ performs behavior $b$}) = 
    \frac{e^{\beta \ledger}}{\sum_{b=1}^B e^{\beta \ledger}},
\end{equation}
\noindent
where $\beta$ is a parameter that adjusts how frequently agents perform 
behaviors with high expected payoffs (larger $\beta$) versus how frequently
agents explore alternative behaviors (smaller $\beta$)~\cite{McElreath2005}. 
Agents probabilistically recieve a payoff of 0 or 1 depending on whether the
bandit paid off, equivalent to a draw from a Bernoulli distribution with 
mean $\pi_b$. For concreteness we denote this probabilistic payoff
$\bandit$. On performing behavior $b$, agent $i$ updates the
corresponding behavior count by 1, $c_{ib} \leftarrow c_{ib} + 1$, and then
the expected payoffs calculated for that behavior are updated with
exponentially-weighted averaging
\begin{equation}
  \ledger \leftarrow \ledger + \frac{\bandit - \ledger}{c_{ib}}.
\end{equation}
\noindent
This procedure continues for $L$ time steps within each generation
(Figure~\ref{fig:schematic}B).

In between generations agents first reproduce via asexual haploid reproduction; 
then new child agents learn from the
previous generation if they are social learners; and finally all agents from the
previous generation then die off (Figure~\ref{fig:schematic}C). 
$N$ reproducers are selected with replacement over $N$ independent draws, 
biased by performance:
\begin{equation}
  \Pr(\text{Agent $i$ is chosen to reproduce in one draw}) = \frac{\pi_i}{\sum_{i=1}^N \pi_i}.
\end{equation}
\noindent
A child inherits its parent's social learning trait $s_i$ without mutation.
A social learner child with $s_i = 1$ learns from a teacher from its parent's
generation, including possibly their parent (oblique learning). 
A child selects its teacher by first selecting $N_T$ prospective
teachers from the population completely at random, then selecting the one with
highest net payoffs $\pi_i$ as their teacher, with ties broken randomly ($N_T =
5$ in the main text; \mt{Other values will be tested in the supplement}). While it
is somewhat arbitrary to first subset $N_T$, this is a conservative choice that
represnts the fact that access to the entire population is not generally guaranteed.
Social learners adopt their chosen teacher's ledger of expected payoffs $\ledger$,
and set all $c_{ib} = 1$ so expected payoffs remain within $[0, 1]$.  Asocial
learners ($s_i = 0$) are initialized with all $c_{ib} = 0$ and $\ledger = 0$ 
(Figure~\ref{fig:schematic}C). The
newly initialized population then begins behavior selection and payoff accumulation
for another $L$ time steps. The evolutionary process continues until $\sum_i s_i
= 1$ or $\sum_i s_i = 0$ \mt{I believe all trials will eventually end like this, so
in the next set of model runs I will not allow early termination before one of these
conditions are met}.

\begin{table}[h]
    \caption{Outcome variables.}
    \label{tab:outcomeVariables}
    \centering %\hspace{-3em}
    \begin{tabular}{cp{4.25in}p{0.85in}} \toprule

        Symbol & Description & Values \\ 

        \midrule  

        $\meansl$ & Mean social learning prevalence over agents and trials
                  & $\in [0.0, 1.0]$ \\

        $\meanpi / L$ & Mean payoffs accumulated in a generation normalized by
        lifespan & $\in [0.0, 1.0]$ \\

        $\meanT / L$ & Mean number of generations to convergence & 20k / 8 max. \\
        \bottomrule
    \end{tabular}
\end{table}

To analyze the effect of the four principal uncertainty factors, we systematically
varied their values and observed three key outcome variables aggregated across 1000
trial simulations, and across all agents when appropriate. We varied $u \in \{0.0,
0.1, \ldots, 1.0\}$; $\pilow \in \{0.1, 0.45, 0.8\}$; $B \in \{2, 4, 10\}$; and $L
\in \{1,2,4,8\}$ for $B=2$ and $L \in \{1,B/2,B,2B\}$ for $B=4,10$.  We observed
three outcome measures: (1) $\meansl$, the average value of $s_i$ over all agents
and trials; (2) $\meanpi / L$, the average net payoffs at simulation end across
agents and trials, normalized by lifespan; and (3) $\meanT / L$, 
or the mean number of time steps to fixation normalized by
lifespan\footnote{Equivalent to the mean number of generations to fixation}
(fixation means $\sum_i s_i = 0 \text{ or } 1$). \mt{Make sure to qualitatively
introduce these and their evolutionary importance in the intro.}
We analyze outcomes by plotting $\meansl$ on the y-axis and environmental
variation is on x-axis since we theoretically expect that $\meansl$ will 
decrease monotonically from 1 to 0 as $u$ increases. We expected and observed
that the other three uncertainty factors shift the value of $u$ at which 
$\meansl$ starts to decrease, and how rapidly $\meansl$ decreases. Note that the
hypothesis-testing concept of significance is meaningless here because we could
make any small outcome difference ``significant'' by running more simulation trials.
We instead study the patterns of outcome 
variables over systematically varied uncertainty factor values. 
Our model was implemented in the Julia programming language~\cite{Bezanson2017} 
using the Agents.jl agent-based modeling library~\cite{Datseris2022} and run
on the Sherlock supercomputing cluster at Stanford University. Model code and
data are publicly available on GitHub\footnote{\url{https://github.com/mt-digital/UncMod}} 
and OSF.io\footnote{\mt{TODO}}.


\section{Analysis}

Several patterns emerged in changes to the shape of social learning suppression over
environmental variability, $u$, across different uncertainty contexts defined by the
payoff ambiguity, selection set size, and effective lifespan
(Figure~\ref{fig:mainResults}). When payoff ambiguity and selection set size were
both small ($\pilow = 0.1$ and $B=2$) social learning only evolved when $u$ is small
since individual learning is relatively efficient in this case
(Figure~\ref{fig:mainResults}, upper left). As $\pilow$ increased the suppression of
$\meansl$ flattened over $u$, which was more pronounced for smaller $L$ (rows of
Figure~\ref{fig:mainResults}).  When $L$ is smaller, agents have fewer opportunities
to learn through trial and error, and so it is more difficult for them to discern
which strategy, social or asocial, is optimal. Increased selection set size led to
both a shift in the critical value of $u$ when $\meansl$ begins to decrease, and
flattened social learning suppression. This is because possibly misleading 
social information is less consequential due
to (1) larger $B$ is a more difficult problem for trial-and-error learning, and
(2) a misidentified optimal behavior is weighted less relative to smaller
$B$, meaning agents are more likely to explore behaviors identified as non-optimal
when $B$ is larger (columns of Figure~\ref{fig:mainResults}).  \mt{Support this by calculating the difference between weights when B=2 and B=10 and a social learner learns outdated information}
Increased $B$ led to more pronounced flattening of social learning suppression when
$L$ was small, apparently again because of the limited data available to evolution,
making individual outcomes more path dependent.

\begin{figure}
  \caption{Social learning prevalence (y-axes) monotonically decreases as 
  environmental variabilty, $u$, increases (x-axes) in most uncertainty contexts. 
  Other uncertainty values $\pilow$ (rows), $B$ (columns), and $L$ (keys)
  shift and flatten the decrease from all-social-learner populations to all-asocial 
  populations.}
  \label{fig:mainResults}
  \centering
    \includegraphics[width=\textwidth]{Figures/mainResultsPlots.pdf}
\end{figure}

Social learning evolution was correlated with differences in population-level
expected payoffs for social versus asocial learning. Furthermore, we found that mean
payoffs realized by our simulated agents track either the expected social learning
payoffs, $\meansoc$, or the expected asocial learning payoffs, $\meanasoc$.
$\meansoc$ and $\meanasoc$ were calculated across uncertainty contexts by
initializing the model to be all social or all individual learners then running the
model for 100 generations for social learning and across 1000 one-generation trials
for individual learning.  Simulated evolutionary mean payoffs across agents and
trials, $\meanpi$, are initially higher than expected asocial learning, $\meanasoc$,
and comparable to expected social learning, $\meansoc$, in most cases 
(Figure~\ref{fig:payoffs}).  But as social learning becomes less beneficial due to
increased environmental variability, $\meansoc$ and $\meanpi$ approach $\meanasoc$.
It appears agents tend
to be conservative as expected social and asocial learning payoffs approach each
other---the transition to $\meansl = 0$ often happens although social
learning is still theoretically optimal. When $L=1$, $\meanpi$ can be greater than
$\meansoc$ because asocial learners at the beginning of the simulation help to 
pass on less misleading information on average, so that the cultural information passed on by
eventual all-social learner populations have more noise. This noise further reduces the
weighting of the previous generation's observed optimal payoff if it changes between
generations enabling agents to more efficiently explore alternatives before focusing
on a misidentified optimal behavior \mt{I think we could calculate this if it were
valuable}. 

\begin{figure}
  \caption{Mean payoff normalized for lifespan (y-axes, solid lines with circles)
    generally track either expected all-social-learners payoff ($\meansoc$, dash-dot
    line with diamonds) or expected all-asocial payoff ($\meanasoc$, long dash
    horizontal lines) as $u$ increases (x-axes). When $\meansl$ goes from 1 to
  0 is when social learning payoffs approach individual learning payoffs due to
increased environmental variability.} 
  \label{fig:payoffs}
\centering
    \includegraphics[width=0.75\textwidth]{Figures/meanNetPayoffs.pdf}
\end{figure}

The number of generations to reach fixation increased
when expected social learning payoffs approached expected asocial payoffs, i.e.,
when $\meansl$ becomes suppressed (Figure~\ref{fig:steps}). Recall a fixated
state is when $\sum_i s_i = 0 \text{ or } 1$.  This supports our explanation that
the supression of social learning begins when individual simulation outcomes are
more path dependent and susceptible to end in a non-optimal state since simulations
in these conditions also take longer to reach fixation.  For example, when
$\pilow=0.1$, $B=4$, and $L=8$, we see a sharp increase in $\meanT$ when social learning
suppression begins; but when $L=1$, social learning suppression is flatter
than for $L=8$, and  correspondingly its $\meanT$ is not as strongly peaked, but is
elevated across the $u$ (Figure~\ref{fig:steps} upper left).  In some cases $L=1$
trials reached fixation faster than $L=8$ or $L=20$; larger $L$ had larger peak $\meanT/L$
than smaller $L$, for example, both for the cases shown in Figure~\ref{fig:steps}
and across all settings tested \mt{TODO: add full figure to SI}. 
However, we also see situations where $L=1$ and fixation takes longer than $L=8$.
This is especially pronounced for
$\pilow = 0.45$ where social learning suppression is much flatter for $L=1$
(Figure~\ref{fig:steps}, bottom row). In general, then, a sharper peak in $\meanT$
indicates a sharper suppression of social learning over $u$ and a flatter peak,
elevated over more values of $u$, indicates greater path dependence and more 
drift overall.


\begin{figure}
  \caption{Average number of generations ($\meanT/L$, y-axes) for populations to fixate
  where agents either all become social or all become asocial learners. As
  $u$ increases (x-axes) models take longer to fixate due to drift, up to a point,
but then fixation accelerates as individual learning becomes more obviously 
superior. Increased $\pilow$ increases time to fixate; both $\pilow$ and $B$
shift peak time to fixation over $u$.} 
  \label{fig:steps}
\centering
    \includegraphics[width=0.75\textwidth]{Figures/stepResultsPlots.pdf}
\end{figure}

\section{Discussion}

By identifying four common uncertainty dimensions we developed a model that predicts
how different forms of uncertainty might interact to foster or suppress social
learning.  The model reproduced classic predictions that social learning can yield
higher payoffs than individual learning when uncertainty is sufficiently limited.
By disambiguating and organizing various operationalizations of uncertainty, 
we developed a more nuanced theoretical understanding of which forms of
uncertainty interact in which ways to make social learning more or less beneficial
compared to asocial learning.  Note that in many cases we made modeling choices that
excluded some important mechanisms of social learning; for example we assumed
success-biased learning, although conformity is theoretically important as
well~\cite{Muthukrishna2016a,Smaldino2018b}. Still, we believe our model
advances theoretical parsimony; and the software implementation is designed to
facilitate testing alternate modeling choices. 

\paragraph{Disc. 2: Foundation for studying social learning evolution with group
structure or other topics(PAUL, JAMIE, CRISTINA)}~\cite{Katsnelson2014}.

Our modeling approach may have applications to developing social artificial
intelligence. Much effort has gone in to developing the cognitive machinery to
realize multi-agent reinforcement learning
systems~\cite{Sandholm1996,Ndousse2021,Gronauer2022}, with some work explicitly
incorporating cultural evolutionary approaches to social learning~\cite{Jaques2019}.
However, it seems there are a lack of methods for adapting to environmental
uncertainty, or determining when social learning is actually advantageous under
different uncertainty conditions. Our model dynamics could be used as an
evolutionary algorithm to update AI agents' social learning strategy without
explicit knowledge of, or need to approximate, the underlying uncertainty structure.
Our model could also predict the expected benefit of social learning, which may be
inherently costly in artificial multi-agent learning systems, and how long the
translated evolutionary algorithm may take to converge.



\bibliographystyle{apacite}
% \bibliography{/Users/mt/workspace/Writing/library.bib}
\bibliography{this.bib}

\appendix


\section{Appendix}

\subsection{Convergence information}

\begin{table}[h]
  \caption{Max. iterations = 5000.}
  \label{tab:convergence}
  \centering
  \begin{tabular}{cccc} \toprule
    $B$ & \# not at fixation & \# total series & Pct. not fixated \\
    \midrule  
    2  & 0  & 132000 & 0.0 \% \\
    4  & 0  & 132000 & 0.0 \% \\
    10 & 44 & 132000 & 0.00033  \% \\
    \bottomrule
  \end{tabular} 
\end{table}

\end{document}
